use std::collections::HashSet;
use split_expanded_lib::{DeclarationItem};

mod rabinkarp { use alloc :: { sync :: Arc , vec , vec :: Vec } ; use crate :: { packed :: pattern :: Patterns , util :: search :: Match , PatternID } ; # [doc = " The type of the rolling hash used in the Rabin-Karp algorithm."] type Hash = usize ; # [doc = " The number of buckets to store our patterns in. We don't want this to be"] # [doc = " too big in order to avoid wasting memory, but we don't want it to be too"] # [doc = " small either to avoid spending too much time confirming literals."] # [doc = ""] # [doc = " The number of buckets MUST be a power of two. Otherwise, determining the"] # [doc = " bucket from a hash will slow down the code considerably. Using a power"] # [doc = " of two means `hash % NUM_BUCKETS` can compile down to a simple `and`"] # [doc = " instruction."] const NUM_BUCKETS : usize = 64 ; # [doc = " An implementation of the Rabin-Karp algorithm. The main idea of this"] # [doc = " algorithm is to maintain a rolling hash as it moves through the input, and"] # [doc = " then check whether that hash corresponds to the same hash for any of the"] # [doc = " patterns we're looking for."] # [doc = ""] # [doc = " A draw back of naively scaling Rabin-Karp to multiple patterns is that"] # [doc = " it requires all of the patterns to be the same length, which in turn"] # [doc = " corresponds to the number of bytes to hash. We adapt this to work for"] # [doc = " multiple patterns of varying size by fixing the number of bytes to hash"] # [doc = " to be the length of the smallest pattern. We also split the patterns into"] # [doc = " several buckets to hopefully make the confirmation step faster."] # [doc = ""] # [doc = " Wikipedia has a decent explanation, if a bit heavy on the theory:"] # [doc = " https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm"] # [doc = ""] # [doc = " But ESMAJ provides something a bit more concrete:"] # [doc = " https://www-igm.univ-mlv.fr/~lecroq/string/node5.html"] pub (crate) struct RabinKarp { # [doc = " The patterns we're searching for."] patterns : Arc < Patterns > , # [doc = " The order of patterns in each bucket is significant. Namely, they are"] # [doc = " arranged such that the first one to match is the correct match. This"] # [doc = " may not necessarily correspond to the order provided by the caller."] # [doc = " For example, if leftmost-longest semantics are used, then the patterns"] # [doc = " are sorted by their length in descending order. If leftmost-first"] # [doc = " semantics are used, then the patterns are sorted by their pattern ID"] # [doc = " in ascending order (which corresponds to the caller's order)."] buckets : Vec < Vec < (Hash , PatternID) > > , # [doc = " The length of the hashing window. Generally, this corresponds to the"] # [doc = " length of the smallest pattern."] hash_len : usize , # [doc = " The factor to subtract out of a hash before updating it with a new"] # [doc = " byte."] hash_2pow : usize , } # [automatically_derived] impl :: core :: clone :: Clone for RabinKarp { # [inline] fn clone (& self) -> RabinKarp { RabinKarp { patterns : :: core :: clone :: Clone :: clone (& self . patterns) , buckets : :: core :: clone :: Clone :: clone (& self . buckets) , hash_len : :: core :: clone :: Clone :: clone (& self . hash_len) , hash_2pow : :: core :: clone :: Clone :: clone (& self . hash_2pow) , } } } # [automatically_derived] impl :: core :: fmt :: Debug for RabinKarp { # [inline] fn fmt (& self , f : & mut :: core :: fmt :: Formatter) -> :: core :: fmt :: Result { :: core :: fmt :: Formatter :: debug_struct_field4_finish (f , "RabinKarp" , "patterns" , & self . patterns , "buckets" , & self . buckets , "hash_len" , & self . hash_len , "hash_2pow" , & & self . hash_2pow ,) } } impl RabinKarp { # [doc = " Compile a new Rabin-Karp matcher from the patterns given."] # [doc = ""] # [doc = " This panics if any of the patterns in the collection are empty, or if"] # [doc = " the collection is itself empty."] pub (crate) fn new (patterns : & Arc < Patterns >) -> RabinKarp { if ! (patterns . len () >= 1) { :: core :: panicking :: panic ("assertion failed: patterns.len() >= 1") } let hash_len = patterns . minimum_len () ; if ! (hash_len >= 1) { :: core :: panicking :: panic ("assertion failed: hash_len >= 1") } let mut hash_2pow = 1usize ; for _ in 1 .. hash_len { hash_2pow = hash_2pow . wrapping_shl (1) ; } let mut rk = RabinKarp { patterns : Arc :: clone (patterns) , buckets : :: alloc :: vec :: from_elem (:: alloc :: vec :: Vec :: new () , NUM_BUCKETS ,) , hash_len , hash_2pow , } ; for (id , pat) in patterns . iter () { let hash = rk . hash (& pat . bytes () [.. rk . hash_len]) ; let bucket = hash % NUM_BUCKETS ; rk . buckets [bucket] . push ((hash , id)) ; } rk } # [doc = " Return the first matching pattern in the given haystack, begining the"] # [doc = " search at `at`."] pub (crate) fn find_at (& self , haystack : & [u8] , mut at : usize ,) -> Option < Match > { match (& NUM_BUCKETS , & self . buckets . len ()) { (left_val , right_val) => { if ! (* left_val == * right_val) { let kind = :: core :: panicking :: AssertKind :: Eq ; :: core :: panicking :: assert_failed (kind , & * left_val , & * right_val , :: core :: option :: Option :: None ,) ; } } } ; if at + self . hash_len > haystack . len () { return None ; } let mut hash = self . hash (& haystack [at .. at + self . hash_len]) ; loop { let bucket = & self . buckets [hash % NUM_BUCKETS] ; for & (phash , pid) in bucket { if phash == hash { if let Some (c) = self . verify (pid , haystack , at) { return Some (c) ; } } } if at + self . hash_len >= haystack . len () { return None ; } hash = self . update_hash (hash , haystack [at] , haystack [at + self . hash_len]) ; at += 1 ; } } # [doc = " Returns the approximate total amount of heap used by this searcher, in"] # [doc = " units of bytes."] pub (crate) fn memory_usage (& self) -> usize { self . buckets . len () * core :: mem :: size_of :: < Vec < (Hash , PatternID) > > () + self . patterns . len () * core :: mem :: size_of :: < (Hash , PatternID) > () } # [doc = " Verify whether the pattern with the given id matches at"] # [doc = " `haystack[at..]`."] # [doc = ""] # [doc = " We tag this function as `cold` because it helps improve codegen."] # [doc = " Intuitively, it would seem like inlining it would be better. However,"] # [doc = " the only time this is called and a match is not found is when there"] # [doc = " there is a hash collision, or when a prefix of a pattern matches but"] # [doc = " the entire pattern doesn't match. This is hopefully fairly rare, and"] # [doc = " if it does occur a lot, it's going to be slow no matter what we do."] # [cold] fn verify (& self , id : PatternID , haystack : & [u8] , at : usize ,) -> Option < Match > { let pat = self . patterns . get (id) ; if pat . is_prefix (& haystack [at ..]) { Some (Match :: new (id , at .. at + pat . len ())) } else { None } } # [doc = " Hash the given bytes."] fn hash (& self , bytes : & [u8]) -> Hash { match (& self . hash_len , & bytes . len ()) { (left_val , right_val) => { if ! (* left_val == * right_val) { let kind = :: core :: panicking :: AssertKind :: Eq ; :: core :: panicking :: assert_failed (kind , & * left_val , & * right_val , :: core :: option :: Option :: None ,) ; } } } ; let mut hash = 0usize ; for & b in bytes { hash = hash . wrapping_shl (1) . wrapping_add (b as usize) ; } hash } # [doc = " Update the hash given based on removing `old_byte` at the beginning"] # [doc = " of some byte string, and appending `new_byte` to the end of that same"] # [doc = " byte string."] fn update_hash (& self , prev : Hash , old_byte : u8 , new_byte : u8) -> Hash { prev . wrapping_sub ((old_byte as usize) . wrapping_mul (self . hash_2pow)) . wrapping_shl (1) . wrapping_add (new_byte as usize) } } }